# ==============================================================================
# GraphSAGE Training Configuration for Memory R1
# ==============================================================================
# This is the default configuration optimized for quality over speed.
# For production deployment, use production.yaml instead.
# ==============================================================================

# ------------------------------------------------------------------------------
# Feature Extraction Settings
# ------------------------------------------------------------------------------
features:
  # Number of input features per memory node
  # [caused_by_deg, next_event_deg, num_entities, shared_entity_neighbors,
  #  is_cause, is_effect, has_successor]
  dimensions: 7

  # Whether to include entity-derived features (num_entities, shared_entity_neighbors)
  include_entity_features: true

  # Normalization method: 'log' (log1p normalization) or 'standard' (z-score)
  normalization: 'log'

# ------------------------------------------------------------------------------
# Random Walk Generation Settings
# ------------------------------------------------------------------------------
walks:
  # Length of each random walk (number of nodes visited)
  # Longer walks capture more global structure but increase memory usage
  length: 80

  # Number of walks to start from each node
  # More walks = more training signal, but increases computation
  per_node: 10

  # Context window size for skip-gram pairs
  # Nodes within this window of each other are considered co-occurring
  context_window: 10

  # Random seed for reproducibility
  seed: 42

# ------------------------------------------------------------------------------
# Negative Sampling Settings
# ------------------------------------------------------------------------------
negatives:
  # Number of negative samples per positive pair
  # Higher values improve quality but slow training
  per_positive: 5

  # Smoothing exponent for degree-biased sampling
  # 0.75 is the Word2Vec default, reduces bias toward high-degree nodes
  exponent: 0.75

# ------------------------------------------------------------------------------
# Model Architecture Settings
# ------------------------------------------------------------------------------
model:
  # Hidden layer dimension
  hidden_dim: 64

  # Output embedding dimension
  # This will be the structural embedding size for Memory R1
  output_dim: 64

  # Number of GraphSAGE layers (message passing rounds)
  num_layers: 2

  # Dropout rate for regularization (applied after first layer)
  dropout: 0.3

  # Aggregation function: 'mean', 'max', 'lstm'
  # 'mean' is most stable and recommended
  aggregator: 'mean'

  # Whether to apply L2 normalization to output embeddings
  # Required for dot-product similarity in skip-gram objective
  normalize_output: true

# ------------------------------------------------------------------------------
# Training Settings
# ------------------------------------------------------------------------------
training:
  # Learning rate for Adam optimizer
  learning_rate: 0.001

  # Weight decay (L2 regularization)
  weight_decay: 0.0

  # Batch size (number of positive pairs per batch)
  # Larger batches are more stable but require more memory
  batch_size: 512

  # Maximum number of training epochs
  epochs: 100

  # Gradient clipping norm (prevents exploding gradients)
  gradient_clip: 1.0

  # Early stopping patience (epochs without improvement before stopping)
  early_stopping_patience: 10

  # Minimum loss improvement to reset patience counter
  min_delta: 0.0001

  # Random seed for reproducibility
  seed: 42

  # Device: 'cuda' for GPU training (recommended)
  device: 'cuda'

  # Enable Automatic Mixed Precision (AMP) for faster training
  # Note: May cause numerical issues in some cases
  use_amp: false

  # Enable torch.compile for optimized execution (PyTorch 2.0+)
  # Recommended for production training
  use_compile: false

  # Logging frequency (log every N batches)
  log_every: 100

  # Checkpoint frequency (save every N epochs)
  checkpoint_every: 10

# ------------------------------------------------------------------------------
# Validation Settings
# ------------------------------------------------------------------------------
validation:
  # Fraction of positive pairs to use for validation
  val_fraction: 0.1

  # Metrics to compute during validation
  metrics:
    - loss
    - neighbor_similarity
    - link_prediction_auc

  # Whether to run validation every epoch
  validate_every_epoch: true

# ------------------------------------------------------------------------------
# Inference Settings
# ------------------------------------------------------------------------------
inference:
  # Enable torch.compile for faster inference
  use_compile: true

  # Cache computed embeddings
  cache_embeddings: true

  # Batch size for inference (can be larger than training)
  batch_size: 1024

# ------------------------------------------------------------------------------
# Logging Settings
# ------------------------------------------------------------------------------
logging:
  # Logging level: DEBUG, INFO, WARNING, ERROR
  level: 'INFO'

  # Log directory
  log_dir: 'logs'

  # Whether to log to TensorBoard
  use_tensorboard: true

  # Whether to save training curves as JSON
  save_metrics_json: true

# ------------------------------------------------------------------------------
# Paths Settings
# ------------------------------------------------------------------------------
paths:
  # Data directories
  raw_data: 'data/raw'
  processed_data: 'data/processed'
  walks_data: 'data/walks'

  # Output directories
  checkpoints: 'checkpoints'
  logs: 'logs'
  exports: 'exports'
