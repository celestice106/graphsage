{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GDS-Style GraphSAGE Training Pipeline\n",
    "\n",
    "**A Hands-On Guide to Random Walk Co-occurrence Training**\n",
    "\n",
    "---\n",
    "\n",
    "## What You Will Build\n",
    "\n",
    "This notebook teaches you to build a complete GraphSAGE training pipeline inspired by Neo4j Graph Data Science (GDS). You will implement:\n",
    "\n",
    "1. **Data Pipeline**: Raw graph â†’ Processed PyG dataset\n",
    "2. **Feature Extraction**: Computing structural node features\n",
    "3. **Random Walk Generation**: The theory and implementation\n",
    "4. **Pair Sampling**: Extracting co-occurrence pairs from walks\n",
    "5. **Negative Sampling**: Degree-biased sampling strategy\n",
    "6. **Training Loop**: Skip-gram loss with scale factor\n",
    "\n",
    "By the end, you'll understand every component of our training pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Data Pipeline\n",
    "\n",
    "## Memory Bank System With Graph Structure\n",
    "\n",
    "The design of our GraphSAGE model is tightly coupled with the memory bank system from Memory R1. \n",
    "\n",
    "A full graph contains 2 types of nodes: **Memory nodes** and **Entity nodes**, and 3 types of edges: **MENTION**, **CAUSED_BY**, **NEXT_EVENT**.\n",
    "\n",
    "## From Raw Graph to Training-Ready Dataset\n",
    "\n",
    "### The Data Flow\n",
    "\n",
    "```\n",
    "Raw Data (Neo4j, JSON, etc.)\n",
    "        â†“\n",
    "Full Graph (Memory + Entity nodes)\n",
    "        â†“\n",
    "Memory-Only View (extract subgraph)\n",
    "        â†“\n",
    "Feature Extraction (7 dimensions)\n",
    "        â†“\n",
    "PyTorch Geometric Dataset\n",
    "```\n",
    "\n",
    "### Why Extract Memory-Only View?\n",
    "\n",
    "Our full graph has:\n",
    "- **Memory nodes**: The events we want embeddings for\n",
    "- **Entity nodes**: People, places, things (auxiliary)\n",
    "\n",
    "For GraphSAGE training, we only need memory nodes. Entity connections are captured in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 1: Building the Data Pipeline\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: Define Graph Structure\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RawGraph:\n",
    "    \"\"\"\n",
    "    Raw graph data structure.\n",
    "    \n",
    "    In production, this comes from Neo4j, JSON files, or databases.\n",
    "    Here we define the structure for synthetic generation.\n",
    "    \"\"\"\n",
    "    memory_ids: List[str]           # List of memory node IDs\n",
    "    entity_ids: List[str]           # List of entity node IDs\n",
    "    caused_by_edges: List[Tuple[str, str]]   # (source, target) - causal\n",
    "    next_event_edges: List[Tuple[str, str]]  # (source, target) - temporal\n",
    "    mention_edges: List[Tuple[str, str]]     # (memory, entity) - references\n",
    "\n",
    "print(\"Graph structure defined!\")\n",
    "print(\"\\nEdge types in Memory R1:\")\n",
    "print(\"  1. caused_by:   Memory A caused Memory B\")\n",
    "print(\"  2. next_event:  Memory A happened before Memory B\")\n",
    "print(\"  3. mention:     Memory mentions Entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Synthetic Graph Generation\n",
    "# =============================================================================\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_synthetic_graph(\n",
    "    num_memories: int = 100,\n",
    "    num_entities: int = 20,\n",
    "    causal_density: float = 0.02,\n",
    "    temporal_density: float = 0.03,\n",
    "    mention_density: float = 0.1,\n",
    "    seed: int = 42\n",
    ") -> RawGraph:\n",
    "    \"\"\"\n",
    "    Generate a synthetic graph for testing.\n",
    "    \n",
    "    Args:\n",
    "        num_memories: Number of memory nodes\n",
    "        num_entities: Number of entity nodes\n",
    "        causal_density: Probability of causal edge between any two memories\n",
    "        temporal_density: Probability of temporal edge\n",
    "        mention_density: Probability of memory mentioning entity\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        RawGraph with all edges\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create node IDs\n",
    "    memory_ids = [f\"memory_{i}\" for i in range(num_memories)]\n",
    "    entity_ids = [f\"entity_{i}\" for i in range(num_entities)]\n",
    "    \n",
    "    # Generate causal edges (directed: cause â†’ effect)\n",
    "    caused_by_edges = []\n",
    "    for i in range(num_memories):\n",
    "        for j in range(i + 1, num_memories):  # Only forward edges\n",
    "            if random.random() < causal_density:\n",
    "                caused_by_edges.append((memory_ids[i], memory_ids[j]))\n",
    "    \n",
    "    # Generate temporal edges (directed: earlier â†’ later)\n",
    "    next_event_edges = []\n",
    "    for i in range(num_memories):\n",
    "        for j in range(i + 1, min(i + 10, num_memories)):  # Local temporal\n",
    "            if random.random() < temporal_density:\n",
    "                next_event_edges.append((memory_ids[i], memory_ids[j]))\n",
    "    \n",
    "    # Generate mention edges (memory â†’ entity)\n",
    "    mention_edges = []\n",
    "    for mem_id in memory_ids:\n",
    "        for ent_id in entity_ids:\n",
    "            if random.random() < mention_density:\n",
    "                mention_edges.append((mem_id, ent_id))\n",
    "    \n",
    "    return RawGraph(\n",
    "        memory_ids=memory_ids,\n",
    "        entity_ids=entity_ids,\n",
    "        caused_by_edges=caused_by_edges,\n",
    "        next_event_edges=next_event_edges,\n",
    "        mention_edges=mention_edges\n",
    "    )\n",
    "\n",
    "# Generate example graph\n",
    "raw_graph = generate_synthetic_graph(num_memories=100, num_entities=20)\n",
    "\n",
    "print(\"Synthetic Graph Generated!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Memory nodes:     {len(raw_graph.memory_ids)}\")\n",
    "print(f\"Entity nodes:     {len(raw_graph.entity_ids)}\")\n",
    "print(f\"Causal edges:     {len(raw_graph.caused_by_edges)}\")\n",
    "print(f\"Temporal edges:   {len(raw_graph.next_event_edges)}\")\n",
    "print(f\"Mention edges:    {len(raw_graph.mention_edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 3: Extract Memory-Only View\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class MemoryGraphView:\n",
    "    \"\"\"\n",
    "    Memory-only subgraph for GraphSAGE training.\n",
    "    \n",
    "    This is the 'view' we train on - only memory nodes,\n",
    "    with edges derived from causal + temporal relationships.\n",
    "    \"\"\"\n",
    "    num_nodes: int\n",
    "    node_ids: List[str]              # Original memory IDs\n",
    "    id_to_idx: Dict[str, int]        # Map ID â†’ index\n",
    "    edge_index: torch.Tensor         # [2, num_edges] in PyG format\n",
    "    edge_types: List[str]            # Type of each edge\n",
    "\n",
    "\n",
    "def extract_memory_view(raw_graph: RawGraph, undirected: bool = True) -> MemoryGraphView:\n",
    "    \"\"\"\n",
    "    Extract memory-only subgraph from full graph.\n",
    "    \n",
    "    Args:\n",
    "        raw_graph: Full graph with all node types\n",
    "        undirected: If True, add reverse edges (recommended for GNN)\n",
    "    \n",
    "    Returns:\n",
    "        MemoryGraphView ready for training\n",
    "    \"\"\"\n",
    "    # Create mapping: memory_id â†’ index\n",
    "    node_ids = raw_graph.memory_ids\n",
    "    id_to_idx = {mid: idx for idx, mid in enumerate(node_ids)}\n",
    "    \n",
    "    # Collect edges (memory-to-memory only)\n",
    "    edges = []\n",
    "    edge_types = []\n",
    "    \n",
    "    # Add causal edges\n",
    "    for src, tgt in raw_graph.caused_by_edges:\n",
    "        if src in id_to_idx and tgt in id_to_idx:\n",
    "            edges.append((id_to_idx[src], id_to_idx[tgt]))\n",
    "            edge_types.append('caused_by')\n",
    "            if undirected:\n",
    "                edges.append((id_to_idx[tgt], id_to_idx[src]))\n",
    "                edge_types.append('caused_by')\n",
    "    \n",
    "    # Add temporal edges\n",
    "    for src, tgt in raw_graph.next_event_edges:\n",
    "        if src in id_to_idx and tgt in id_to_idx:\n",
    "            edges.append((id_to_idx[src], id_to_idx[tgt]))\n",
    "            edge_types.append('next_event')\n",
    "            if undirected:\n",
    "                edges.append((id_to_idx[tgt], id_to_idx[src]))\n",
    "                edge_types.append('next_event')\n",
    "    \n",
    "    # Convert to PyG format\n",
    "    if edges:\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "    \n",
    "    return MemoryGraphView(\n",
    "        num_nodes=len(node_ids),\n",
    "        node_ids=node_ids,\n",
    "        id_to_idx=id_to_idx,\n",
    "        edge_index=edge_index,\n",
    "        edge_types=edge_types\n",
    "    )\n",
    "\n",
    "# Extract view\n",
    "memory_view = extract_memory_view(raw_graph, undirected=True)\n",
    "\n",
    "print(\"Memory-Only View Extracted!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Nodes:  {memory_view.num_nodes}\")\n",
    "print(f\"Edges:  {memory_view.edge_index.shape[1]} (undirected, so 2x original)\")\n",
    "print(f\"\\nEdge index shape: {memory_view.edge_index.shape}\")\n",
    "print(f\"Format: [2, num_edges] where row 0 = source, row 1 = target\")\n",
    "print(f\"\\nFirst 5 edges:\")\n",
    "for i in range(min(5, memory_view.edge_index.shape[1])):\n",
    "    src, tgt = memory_view.edge_index[:, i].tolist()\n",
    "    print(f\"  {memory_view.node_ids[src]} â†’ {memory_view.node_ids[tgt]} ({memory_view.edge_types[i]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 2: Feature Extraction\n",
    "\n",
    "## Computing the 7-Dimensional Node Features\n",
    "\n",
    "GraphSAGE needs initial node features. We compute 7 structural features:\n",
    "\n",
    "| Index | Feature | Description | Type |\n",
    "|-------|---------|-------------|------|\n",
    "| 0 | `caused_by_in_degree` | Incoming causal edges | Continuous |\n",
    "| 1 | `next_event_out_degree` | Outgoing temporal edges | Continuous |\n",
    "| 2 | `num_entities` | Entities mentioned | Continuous |\n",
    "| 3 | `shared_entity_neighbors` | Neighbors sharing entities | Continuous |\n",
    "| 4 | `is_cause` | Has outgoing causal edges? | Binary |\n",
    "| 5 | `is_effect` | Has incoming causal edges? | Binary |\n",
    "| 6 | `has_successor` | Has temporal successors? | Binary |\n",
    "\n",
    "### Why These Features?\n",
    "\n",
    "They capture the **structural role** of each memory:\n",
    "- Root causes: high `is_cause`, low `is_effect`\n",
    "- Final effects: low `is_cause`, high `is_effect`\n",
    "- Hub memories: high degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Feature Extraction Implementation\n",
    "# =============================================================================\n",
    "\n",
    "def compute_node_features(\n",
    "    raw_graph: RawGraph,\n",
    "    memory_view: MemoryGraphView\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute 7-dimensional features for each memory node.\n",
    "    \n",
    "    Args:\n",
    "        raw_graph: Original graph with all edges\n",
    "        memory_view: Memory-only view with index mapping\n",
    "    \n",
    "    Returns:\n",
    "        features: Tensor of shape [num_nodes, 7]\n",
    "    \"\"\"\n",
    "    num_nodes = memory_view.num_nodes\n",
    "    id_to_idx = memory_view.id_to_idx\n",
    "    \n",
    "    # Initialize feature arrays\n",
    "    caused_by_in_degree = np.zeros(num_nodes)\n",
    "    next_event_out_degree = np.zeros(num_nodes)\n",
    "    num_entities = np.zeros(num_nodes)\n",
    "    is_cause = np.zeros(num_nodes)\n",
    "    is_effect = np.zeros(num_nodes)\n",
    "    has_successor = np.zeros(num_nodes)\n",
    "    \n",
    "    # Count causal edges\n",
    "    for src, tgt in raw_graph.caused_by_edges:\n",
    "        if src in id_to_idx and tgt in id_to_idx:\n",
    "            src_idx, tgt_idx = id_to_idx[src], id_to_idx[tgt]\n",
    "            caused_by_in_degree[tgt_idx] += 1  # Target receives cause\n",
    "            is_cause[src_idx] = 1              # Source is a cause\n",
    "            is_effect[tgt_idx] = 1             # Target is an effect\n",
    "    \n",
    "    # Count temporal edges\n",
    "    for src, tgt in raw_graph.next_event_edges:\n",
    "        if src in id_to_idx and tgt in id_to_idx:\n",
    "            src_idx = id_to_idx[src]\n",
    "            next_event_out_degree[src_idx] += 1\n",
    "            has_successor[src_idx] = 1\n",
    "    \n",
    "    # Count entity mentions\n",
    "    for mem_id, ent_id in raw_graph.mention_edges:\n",
    "        if mem_id in id_to_idx:\n",
    "            num_entities[id_to_idx[mem_id]] += 1\n",
    "    \n",
    "    # Compute shared entity neighbors (simplified)\n",
    "    # For each memory, count how many neighbors share at least one entity\n",
    "    memory_entities = {mid: set() for mid in raw_graph.memory_ids}\n",
    "    for mem_id, ent_id in raw_graph.mention_edges:\n",
    "        if mem_id in memory_entities:\n",
    "            memory_entities[mem_id].add(ent_id)\n",
    "    \n",
    "    shared_entity_neighbors = np.zeros(num_nodes)\n",
    "    # Get neighbors for each node\n",
    "    neighbors = {i: set() for i in range(num_nodes)}\n",
    "    for i in range(memory_view.edge_index.shape[1]):\n",
    "        src, tgt = memory_view.edge_index[:, i].tolist()\n",
    "        neighbors[src].add(tgt)\n",
    "    \n",
    "    for idx in range(num_nodes):\n",
    "        node_id = memory_view.node_ids[idx]\n",
    "        node_entities = memory_entities.get(node_id, set())\n",
    "        if node_entities:\n",
    "            for neighbor_idx in neighbors[idx]:\n",
    "                neighbor_id = memory_view.node_ids[neighbor_idx]\n",
    "                neighbor_entities = memory_entities.get(neighbor_id, set())\n",
    "                if node_entities & neighbor_entities:  # Intersection\n",
    "                    shared_entity_neighbors[idx] += 1\n",
    "    \n",
    "    # Apply log normalization to continuous features\n",
    "    # log1p(x) = log(1 + x), handles zeros gracefully\n",
    "    caused_by_in_degree = np.log1p(caused_by_in_degree)\n",
    "    next_event_out_degree = np.log1p(next_event_out_degree)\n",
    "    num_entities = np.log1p(num_entities)\n",
    "    shared_entity_neighbors = np.log1p(shared_entity_neighbors)\n",
    "    \n",
    "    # Stack into feature matrix\n",
    "    features = np.stack([\n",
    "        caused_by_in_degree,     # 0\n",
    "        next_event_out_degree,   # 1\n",
    "        num_entities,            # 2\n",
    "        shared_entity_neighbors, # 3\n",
    "        is_cause,                # 4\n",
    "        is_effect,               # 5\n",
    "        has_successor,           # 6\n",
    "    ], axis=1)\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "# Compute features\n",
    "features = compute_node_features(raw_graph, memory_view)\n",
    "\n",
    "print(\"Feature Extraction Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Feature shape: {features.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "feature_names = [\n",
    "    'caused_by_in_degree', 'next_event_out_degree', 'num_entities',\n",
    "    'shared_entity_neighbors', 'is_cause', 'is_effect', 'has_successor'\n",
    "]\n",
    "for i, name in enumerate(feature_names):\n",
    "    col = features[:, i]\n",
    "    print(f\"  {name:25s}: min={col.min():.2f}, max={col.max():.2f}, mean={col.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    ax = axes[i]\n",
    "    ax.hist(features[:, i].numpy(), bins=20, edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "# Hide empty subplot\n",
    "axes[7].axis('off')\n",
    "\n",
    "plt.suptitle('Node Feature Distributions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice:\")\n",
    "print(\"   - Continuous features (0-3) are log-normalized\")\n",
    "print(\"   - Binary features (4-6) are 0 or 1\")\n",
    "print(\"   - This mix helps GraphSAGE learn structural roles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 3: Random Walk Theory\n",
    "\n",
    "## The Core Idea Behind GDS-Style Training\n",
    "\n",
    "### What is a Random Walk?\n",
    "\n",
    "A random walk is a sequence of nodes where each step moves to a **random neighbor**:\n",
    "\n",
    "```\n",
    "Start at node A\n",
    "    â†’ randomly pick neighbor B\n",
    "    â†’ randomly pick neighbor of B â†’ C\n",
    "    â†’ randomly pick neighbor of C â†’ D\n",
    "    â†’ ... continue for L steps\n",
    "    \n",
    "Result: Walk = [A, B, C, D, ...]\n",
    "```\n",
    "\n",
    "### Why Random Walks?\n",
    "\n",
    "Random walks capture **structural proximity**:\n",
    "- Nodes appearing together in walks are **structurally close**\n",
    "- The more often two nodes co-occur, the more similar they are\n",
    "\n",
    "### The Key Insight (from Node2Vec/DeepWalk)\n",
    "\n",
    "Treat walks like **sentences** and nodes like **words**:\n",
    "- Word2Vec: Words appearing in same context are similar\n",
    "- Node2Vec: Nodes appearing in same walk are similar\n",
    "\n",
    "This is why we use **skip-gram loss** - same objective as Word2Vec!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Random Walk Generator Implementation\n",
    "# =============================================================================\n",
    "\n",
    "class RandomWalkGenerator:\n",
    "    \"\"\"\n",
    "    Generate random walks on a graph.\n",
    "    \n",
    "    This is the foundation of GDS-style training:\n",
    "    1. Generate many random walks\n",
    "    2. Extract co-occurring node pairs\n",
    "    3. Train embeddings with skip-gram loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        edge_index: torch.Tensor,\n",
    "        num_nodes: int,\n",
    "        walk_length: int = 80,\n",
    "        walks_per_node: int = 10,\n",
    "        seed: int = 42\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            edge_index: [2, num_edges] tensor\n",
    "            num_nodes: Total number of nodes\n",
    "            walk_length: Steps per walk\n",
    "            walks_per_node: Walks starting from each node\n",
    "            seed: Random seed\n",
    "        \"\"\"\n",
    "        self.num_nodes = num_nodes\n",
    "        self.walk_length = walk_length\n",
    "        self.walks_per_node = walks_per_node\n",
    "        \n",
    "        random.seed(seed)\n",
    "        \n",
    "        # Build adjacency list for fast neighbor lookup\n",
    "        self.neighbors = {i: [] for i in range(num_nodes)}\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            src, tgt = edge_index[:, i].tolist()\n",
    "            self.neighbors[src].append(tgt)\n",
    "        \n",
    "        # Track nodes with no neighbors (isolated)\n",
    "        self.isolated_nodes = [i for i in range(num_nodes) if not self.neighbors[i]]\n",
    "    \n",
    "    def generate_single_walk(self, start_node: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Generate one random walk starting from a node.\n",
    "        \n",
    "        The walk proceeds by:\n",
    "        1. Start at start_node\n",
    "        2. Randomly select a neighbor\n",
    "        3. Move to that neighbor\n",
    "        4. Repeat until walk_length reached or stuck\n",
    "        \"\"\"\n",
    "        walk = [start_node]\n",
    "        current = start_node\n",
    "        \n",
    "        for _ in range(self.walk_length - 1):\n",
    "            neighbors = self.neighbors[current]\n",
    "            if not neighbors:\n",
    "                break  # Stuck at node with no outgoing edges\n",
    "            current = random.choice(neighbors)\n",
    "            walk.append(current)\n",
    "        \n",
    "        return walk\n",
    "    \n",
    "    def generate_all_walks(self) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Generate walks_per_node walks starting from each node.\n",
    "        \n",
    "        Returns:\n",
    "            List of walks, each walk is a list of node indices\n",
    "        \"\"\"\n",
    "        all_walks = []\n",
    "        \n",
    "        for node in range(self.num_nodes):\n",
    "            if node in self.isolated_nodes:\n",
    "                continue  # Skip isolated nodes\n",
    "            \n",
    "            for _ in range(self.walks_per_node):\n",
    "                walk = self.generate_single_walk(node)\n",
    "                if len(walk) > 1:  # Only keep walks with at least 2 nodes\n",
    "                    all_walks.append(walk)\n",
    "        \n",
    "        return all_walks\n",
    "\n",
    "# Generate walks\n",
    "walker = RandomWalkGenerator(\n",
    "    edge_index=memory_view.edge_index,\n",
    "    num_nodes=memory_view.num_nodes,\n",
    "    walk_length=20,      # Shorter for demo\n",
    "    walks_per_node=5,    # Fewer for demo\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "walks = walker.generate_all_walks()\n",
    "\n",
    "print(\"Random Walks Generated!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total walks: {len(walks)}\")\n",
    "print(f\"Walk length (target): {walker.walk_length}\")\n",
    "print(f\"Isolated nodes (skipped): {len(walker.isolated_nodes)}\")\n",
    "\n",
    "# Show example walks\n",
    "print(f\"\\nExample walks:\")\n",
    "for i in range(min(3, len(walks))):\n",
    "    walk = walks[i]\n",
    "    walk_str = ' â†’ '.join([str(n) for n in walk[:8]])\n",
    "    if len(walk) > 8:\n",
    "        walk_str += f' â†’ ... ({len(walk)} total)'\n",
    "    print(f\"  Walk {i+1}: {walk_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize walk statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "walk_lengths = [len(w) for w in walks]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Walk length distribution\n",
    "axes[0].hist(walk_lengths, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=np.mean(walk_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(walk_lengths):.1f}')\n",
    "axes[0].set_xlabel('Walk Length')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Walk Length Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Node visit frequency\n",
    "node_visits = np.zeros(memory_view.num_nodes)\n",
    "for walk in walks:\n",
    "    for node in walk:\n",
    "        node_visits[node] += 1\n",
    "\n",
    "axes[1].hist(node_visits, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Times Visited')\n",
    "axes[1].set_ylabel('Number of Nodes')\n",
    "axes[1].set_title('Node Visit Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key observations:\")\n",
    "print(f\"   - Some walks are shorter (hit dead ends)\")\n",
    "print(f\"   - High-degree nodes are visited more often\")\n",
    "print(f\"   - This is natural and helps training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 4: Pair Sampling (Co-occurrence Extraction)\n",
    "\n",
    "## From Walks to Training Pairs\n",
    "\n",
    "### The Context Window\n",
    "\n",
    "For each node in a walk, we extract pairs with nodes in a **context window**:\n",
    "\n",
    "```\n",
    "Walk: [A, B, C, D, E, F, G]\n",
    "Window size: 2\n",
    "\n",
    "For target C:\n",
    "  Context = [A, B, D, E]  (2 before, 2 after)\n",
    "  Pairs: (C,A), (C,B), (C,D), (C,E)\n",
    "```\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "Nodes appearing in the same window are **structurally proximate**:\n",
    "- They're reachable within a few hops\n",
    "- Random walks naturally capture graph structure\n",
    "- More walks = better coverage of relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Co-occurrence Pair Sampler Implementation\n",
    "# =============================================================================\n",
    "\n",
    "class CooccurrencePairSampler:\n",
    "    \"\"\"\n",
    "    Extract (target, context) pairs from random walks.\n",
    "    \n",
    "    This is the Skip-gram sampling strategy:\n",
    "    - For each node in a walk, pair it with nearby nodes\n",
    "    - These pairs become positive training examples\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context_window: int = 5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context_window: How many nodes before/after to consider as context\n",
    "        \"\"\"\n",
    "        self.context_window = context_window\n",
    "    \n",
    "    def extract_pairs_from_walk(self, walk: List[int]) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Extract all (target, context) pairs from a single walk.\n",
    "        \n",
    "        For each position i in the walk:\n",
    "        - target = walk[i]\n",
    "        - context = walk[i-window : i+window+1], excluding i\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        walk_len = len(walk)\n",
    "        \n",
    "        for i, target in enumerate(walk):\n",
    "            # Define context window boundaries\n",
    "            start = max(0, i - self.context_window)\n",
    "            end = min(walk_len, i + self.context_window + 1)\n",
    "            \n",
    "            # Extract context nodes (excluding target itself)\n",
    "            for j in range(start, end):\n",
    "                if j != i:\n",
    "                    context = walk[j]\n",
    "                    pairs.append((target, context))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def extract_pairs(self, walks: List[List[int]]) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Extract pairs from all walks.\n",
    "        \"\"\"\n",
    "        all_pairs = []\n",
    "        for walk in walks:\n",
    "            pairs = self.extract_pairs_from_walk(walk)\n",
    "            all_pairs.extend(pairs)\n",
    "        return all_pairs\n",
    "\n",
    "# Extract pairs\n",
    "pair_sampler = CooccurrencePairSampler(context_window=3)\n",
    "positive_pairs = pair_sampler.extract_pairs(walks)\n",
    "\n",
    "print(\"Positive Pairs Extracted!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total walks: {len(walks)}\")\n",
    "print(f\"Context window: {pair_sampler.context_window}\")\n",
    "print(f\"Total pairs: {len(positive_pairs):,}\")\n",
    "print(f\"\\nSignal amplification: {len(positive_pairs) / memory_view.edge_index.shape[1]:.1f}x more pairs than edges!\")\n",
    "\n",
    "# Show example pairs\n",
    "print(f\"\\nExample pairs (target, context):\")\n",
    "for i in range(min(5, len(positive_pairs))):\n",
    "    target, context = positive_pairs[i]\n",
    "    print(f\"  ({target}, {context})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the pair extraction process\n",
    "print(\"Pair Extraction Visualization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Take first walk as example\n",
    "example_walk = walks[0][:10]  # First 10 nodes\n",
    "window = 2\n",
    "\n",
    "print(f\"\\nExample walk: {example_walk}\")\n",
    "print(f\"Window size: {window}\")\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "\n",
    "for i, target in enumerate(example_walk):\n",
    "    start = max(0, i - window)\n",
    "    end = min(len(example_walk), i + window + 1)\n",
    "    \n",
    "    context_nodes = [example_walk[j] for j in range(start, end) if j != i]\n",
    "    \n",
    "    # Visual representation\n",
    "    visual = []\n",
    "    for j, node in enumerate(example_walk):\n",
    "        if j == i:\n",
    "            visual.append(f\"[{node}]\")\n",
    "        elif j >= start and j < end:\n",
    "            visual.append(f\"({node})\")\n",
    "        else:\n",
    "            visual.append(f\" {node} \")\n",
    "    \n",
    "    print(f\"Position {i}: {' '.join(visual)}\")\n",
    "    print(f\"           Target={target}, Context={context_nodes}\")\n",
    "    print(f\"           Pairs: {[(target, c) for c in context_nodes]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 5: Negative Sampling\n",
    "\n",
    "## Why Negative Samples Are Essential\n",
    "\n",
    "### The Collapse Problem\n",
    "\n",
    "If we only train on positive pairs (pull similar nodes together), the model could learn:\n",
    "- \"Make ALL embeddings identical\" â†’ Loss = 0!\n",
    "\n",
    "This is called **embedding collapse**. Negative samples prevent it by:\n",
    "- Pushing non-co-occurring nodes apart\n",
    "- Creating contrast between positive and negative\n",
    "\n",
    "### Degree-Biased Sampling\n",
    "\n",
    "Not all negatives are equal. We use **degree-biased sampling** from Word2Vec:\n",
    "\n",
    "```\n",
    "P(node) âˆ degree(node)^0.75\n",
    "```\n",
    "\n",
    "**Why 0.75?**\n",
    "- Exponent = 1.0: Sample exactly proportional to degree (too biased to hubs)\n",
    "- Exponent = 0.0: Uniform sampling (ignores structure)\n",
    "- Exponent = 0.75: Sweet spot (empirically proven by Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Degree-Biased Negative Sampler Implementation\n",
    "# =============================================================================\n",
    "\n",
    "class DegreeBiasedNegativeSampler:\n",
    "    \"\"\"\n",
    "    Sample negative nodes with probability proportional to degree^exponent.\n",
    "    \n",
    "    Uses the alias method for O(1) sampling after O(n) preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        edge_index: torch.Tensor,\n",
    "        num_nodes: int,\n",
    "        exponent: float = 0.75\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            edge_index: Graph edges\n",
    "            num_nodes: Total nodes\n",
    "            exponent: Degree smoothing (0.75 is Word2Vec default)\n",
    "        \"\"\"\n",
    "        self.num_nodes = num_nodes\n",
    "        self.exponent = exponent\n",
    "        \n",
    "        # Compute node degrees\n",
    "        degrees = np.zeros(num_nodes)\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            src = edge_index[0, i].item()\n",
    "            degrees[src] += 1\n",
    "        \n",
    "        # Apply exponent and normalize\n",
    "        # Add small epsilon to avoid zero probability for isolated nodes\n",
    "        weights = (degrees + 1e-10) ** exponent\n",
    "        self.probabilities = weights / weights.sum()\n",
    "        \n",
    "        # Store degrees for analysis\n",
    "        self.degrees = degrees\n",
    "    \n",
    "    def sample(self, num_samples: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Sample negative node indices.\n",
    "        \n",
    "        Args:\n",
    "            num_samples: Number of negatives to sample\n",
    "        \n",
    "        Returns:\n",
    "            Array of sampled node indices\n",
    "        \"\"\"\n",
    "        return np.random.choice(\n",
    "            self.num_nodes,\n",
    "            size=num_samples,\n",
    "            p=self.probabilities,\n",
    "            replace=True\n",
    "        )\n",
    "    \n",
    "    def sample_for_pairs(\n",
    "        self,\n",
    "        num_pairs: int,\n",
    "        num_negatives: int = 5\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Sample negatives for a batch of positive pairs.\n",
    "        \n",
    "        Args:\n",
    "            num_pairs: Number of positive pairs\n",
    "            num_negatives: Negatives per positive pair\n",
    "        \n",
    "        Returns:\n",
    "            Array of shape [num_pairs, num_negatives]\n",
    "        \"\"\"\n",
    "        total_negatives = num_pairs * num_negatives\n",
    "        negatives = self.sample(total_negatives)\n",
    "        return negatives.reshape(num_pairs, num_negatives)\n",
    "\n",
    "# Create sampler\n",
    "neg_sampler = DegreeBiasedNegativeSampler(\n",
    "    edge_index=memory_view.edge_index,\n",
    "    num_nodes=memory_view.num_nodes,\n",
    "    exponent=0.75\n",
    ")\n",
    "\n",
    "print(\"Negative Sampler Created!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Exponent: {neg_sampler.exponent}\")\n",
    "print(f\"Nodes: {neg_sampler.num_nodes}\")\n",
    "print(f\"\\nDegree statistics:\")\n",
    "print(f\"  Min degree: {neg_sampler.degrees.min():.0f}\")\n",
    "print(f\"  Max degree: {neg_sampler.degrees.max():.0f}\")\n",
    "print(f\"  Mean degree: {neg_sampler.degrees.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sampling distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample many times to see distribution\n",
    "samples = neg_sampler.sample(100000)\n",
    "sample_counts = np.bincount(samples, minlength=neg_sampler.num_nodes)\n",
    "\n",
    "# Sort nodes by degree for visualization\n",
    "sorted_indices = np.argsort(neg_sampler.degrees)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Degree distribution\n",
    "axes[0].bar(range(len(sorted_indices)), neg_sampler.degrees[sorted_indices], alpha=0.7)\n",
    "axes[0].set_xlabel('Node (sorted by degree)')\n",
    "axes[0].set_ylabel('Degree')\n",
    "axes[0].set_title('Node Degrees')\n",
    "\n",
    "# Sampling probability\n",
    "axes[1].bar(range(len(sorted_indices)), neg_sampler.probabilities[sorted_indices], alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Node (sorted by degree)')\n",
    "axes[1].set_ylabel('Sampling Probability')\n",
    "axes[1].set_title(f'Sampling Probability (exponent={neg_sampler.exponent})')\n",
    "\n",
    "# Actual samples\n",
    "axes[2].bar(range(len(sorted_indices)), sample_counts[sorted_indices], alpha=0.7, color='green')\n",
    "axes[2].set_xlabel('Node (sorted by degree)')\n",
    "axes[2].set_ylabel('Times Sampled')\n",
    "axes[2].set_title('Actual Sample Distribution (100K samples)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key insight:\")\n",
    "print(\"   - High-degree nodes are sampled more often (harder negatives)\")\n",
    "print(\"   - But the 0.75 exponent prevents extreme bias\")\n",
    "print(\"   - This balance is crucial for good training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different exponents\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "exponents = [0.0, 0.75, 1.0]\n",
    "titles = ['Uniform (exp=0)', 'Word2Vec (exp=0.75)', 'Proportional (exp=1.0)']\n",
    "\n",
    "for ax, exp, title in zip(axes, exponents, titles):\n",
    "    # Compute probabilities for this exponent\n",
    "    weights = (neg_sampler.degrees + 1e-10) ** exp\n",
    "    probs = weights / weights.sum()\n",
    "    \n",
    "    # Sort by degree\n",
    "    sorted_probs = probs[sorted_indices]\n",
    "    \n",
    "    ax.bar(range(len(sorted_probs)), sorted_probs, alpha=0.7)\n",
    "    ax.set_xlabel('Node (sorted by degree)')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Exponent effects:\")\n",
    "print(\"   - exp=0.0:  All nodes equally likely (ignores structure)\")\n",
    "print(\"   - exp=0.75: Balanced - favors high-degree but not extreme\")\n",
    "print(\"   - exp=1.0:  Proportional to degree (too biased to hubs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 6: The Scale Factor (Critical Discovery)\n",
    "\n",
    "## The Problem We Discovered\n",
    "\n",
    "Our first training attempt showed metrics stuck at:\n",
    "```\n",
    "pos_prob: 0.71 (should be ~0.95)\n",
    "neg_prob: 0.50 (should be ~0.10)\n",
    "```\n",
    "\n",
    "### Root Cause: Bounded Dot Products\n",
    "\n",
    "1. Our embeddings are **L2 normalized** (unit vectors)\n",
    "2. Dot product of unit vectors: **-1 â‰¤ dot â‰¤ 1**\n",
    "3. Sigmoid of bounded input:\n",
    "   - `sigmoid(1) = 0.73` â† Maximum!\n",
    "   - `sigmoid(-1) = 0.27` â† Minimum!\n",
    "\n",
    "**The model CANNOT learn** because probabilities are stuck!\n",
    "\n",
    "### Solution: Scale Factor\n",
    "\n",
    "Multiply dot products by a scale factor before sigmoid:\n",
    "\n",
    "```python\n",
    "# Before (broken)\n",
    "loss = -log(sigmoid(dot_product))  # bounded to [0.27, 0.73]\n",
    "\n",
    "# After (fixed)\n",
    "loss = -log(sigmoid(dot_product * 5.0))  # can reach [0.007, 0.993]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the scale factor problem and solution\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"The Scale Factor Problem\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dot product range for normalized vectors\n",
    "dot_products = np.linspace(-1, 1, 100)\n",
    "\n",
    "# Different scale factors\n",
    "scale_factors = [1.0, 3.0, 5.0, 10.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot sigmoid curves\n",
    "for scale in scale_factors:\n",
    "    sigmoid_vals = 1 / (1 + np.exp(-dot_products * scale))\n",
    "    axes[0].plot(dot_products, sigmoid_vals, label=f'scale={scale}', linewidth=2)\n",
    "\n",
    "axes[0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(y=0.73, color='red', linestyle=':', alpha=0.5, label='Old ceiling (0.73)')\n",
    "axes[0].axhline(y=0.27, color='red', linestyle=':', alpha=0.5)\n",
    "axes[0].set_xlabel('Dot Product')\n",
    "axes[0].set_ylabel('Sigmoid Output (Probability)')\n",
    "axes[0].set_title('Effect of Scale Factor on Sigmoid')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Show achievable ranges\n",
    "scale_values = [1.0, 2.0, 3.0, 5.0, 7.0, 10.0]\n",
    "max_probs = [1 / (1 + np.exp(-1 * s)) for s in scale_values]\n",
    "min_probs = [1 / (1 + np.exp(1 * s)) for s in scale_values]\n",
    "\n",
    "axes[1].fill_between(scale_values, min_probs, max_probs, alpha=0.3, color='blue')\n",
    "axes[1].plot(scale_values, max_probs, 'b-o', label='Max achievable', linewidth=2)\n",
    "axes[1].plot(scale_values, min_probs, 'r-o', label='Min achievable', linewidth=2)\n",
    "axes[1].axhline(y=0.95, color='green', linestyle='--', alpha=0.7, label='Target pos_prob')\n",
    "axes[1].axhline(y=0.10, color='green', linestyle='--', alpha=0.7, label='Target neg_prob')\n",
    "axes[1].set_xlabel('Scale Factor')\n",
    "axes[1].set_ylabel('Probability')\n",
    "axes[1].set_title('Achievable Probability Range by Scale Factor')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key insight:\")\n",
    "print(\"   scale=1.0: Can only reach [0.27, 0.73] - TRAINING STUCK!\")\n",
    "print(\"   scale=5.0: Can reach [0.007, 0.993] - TRAINING WORKS!\")\n",
    "print(\"\\n   We use scale_factor=5.0 as default.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Skip-gram Loss with Scale Factor\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGramLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-gram loss with scale factor for normalized embeddings.\n",
    "    \n",
    "    Loss = -log(Ïƒ(s Â· pos_score)) - Î£ log(Ïƒ(-s Â· neg_score))\n",
    "    \n",
    "    Where s is the scale factor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scale_factor: float = 5.0):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        embeddings: torch.Tensor,\n",
    "        targets: torch.Tensor,\n",
    "        contexts: torch.Tensor,\n",
    "        negatives: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute skip-gram loss.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: All node embeddings [num_nodes, dim]\n",
    "            targets: Target node indices [batch_size]\n",
    "            contexts: Context node indices [batch_size]\n",
    "            negatives: Negative indices [batch_size, num_negatives]\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        target_emb = embeddings[targets]      # [batch, dim]\n",
    "        context_emb = embeddings[contexts]    # [batch, dim]\n",
    "        negative_emb = embeddings[negatives]  # [batch, num_neg, dim]\n",
    "        \n",
    "        # Positive scores: dot product of target and context\n",
    "        pos_scores = (target_emb * context_emb).sum(dim=1)  # [batch]\n",
    "        \n",
    "        # Negative scores: dot product of target and each negative\n",
    "        neg_scores = torch.bmm(\n",
    "            negative_emb,\n",
    "            target_emb.unsqueeze(2)\n",
    "        ).squeeze(2)  # [batch, num_neg]\n",
    "        \n",
    "        # CRITICAL: Apply scale factor!\n",
    "        pos_scores = pos_scores * self.scale_factor\n",
    "        neg_scores = neg_scores * self.scale_factor\n",
    "        \n",
    "        # Loss: maximize pos similarity, minimize neg similarity\n",
    "        pos_loss = -F.logsigmoid(pos_scores).mean()\n",
    "        neg_loss = -F.logsigmoid(-neg_scores).mean()\n",
    "        \n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "# Test the loss\n",
    "loss_fn = SkipGramLoss(scale_factor=5.0)\n",
    "\n",
    "# Create dummy data\n",
    "batch_size = 32\n",
    "num_nodes = 100\n",
    "embed_dim = 64\n",
    "num_negatives = 5\n",
    "\n",
    "dummy_embeddings = F.normalize(torch.randn(num_nodes, embed_dim), dim=1)\n",
    "dummy_targets = torch.randint(0, num_nodes, (batch_size,))\n",
    "dummy_contexts = torch.randint(0, num_nodes, (batch_size,))\n",
    "dummy_negatives = torch.randint(0, num_nodes, (batch_size, num_negatives))\n",
    "\n",
    "loss = loss_fn(dummy_embeddings, dummy_targets, dummy_contexts, dummy_negatives)\n",
    "\n",
    "print(\"Skip-gram Loss Test\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Scale factor: {loss_fn.scale_factor}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Negatives per positive: {num_negatives}\")\n",
    "print(f\"Loss value: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 7: Putting It All Together\n",
    "\n",
    "## The Complete Training Pipeline\n",
    "\n",
    "Now let's see how all components work together:\n",
    "\n",
    "```\n",
    "1. Generate/Load Graph\n",
    "        â†“\n",
    "2. Extract Memory View\n",
    "        â†“\n",
    "3. Compute Node Features\n",
    "        â†“\n",
    "4. Generate Random Walks\n",
    "        â†“\n",
    "5. Extract Positive Pairs\n",
    "        â†“\n",
    "6. For each training batch:\n",
    "   a. Sample positive pairs\n",
    "   b. Sample negative nodes\n",
    "   c. Compute embeddings (GraphSAGE forward)\n",
    "   d. Compute skip-gram loss (with scale factor!)\n",
    "   e. Backpropagate and update\n",
    "        â†“\n",
    "7. Evaluate embeddings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Complete Mini Training Example\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simple GraphSAGE model\n",
    "class SimpleGraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Linear(in_dim * 2, hidden_dim)\n",
    "        self.conv2 = nn.Linear(hidden_dim * 2, out_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def aggregate(self, x, edge_index, num_nodes):\n",
    "        \"\"\"Mean aggregation of neighbors.\"\"\"\n",
    "        agg = torch.zeros(num_nodes, x.size(1))\n",
    "        count = torch.zeros(num_nodes, 1)\n",
    "        \n",
    "        src, tgt = edge_index\n",
    "        agg.index_add_(0, tgt, x[src])\n",
    "        count.index_add_(0, tgt, torch.ones(src.size(0), 1))\n",
    "        \n",
    "        return agg / count.clamp(min=1)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        num_nodes = x.size(0)\n",
    "        \n",
    "        # Layer 1\n",
    "        neigh = self.aggregate(x, edge_index, num_nodes)\n",
    "        x = torch.cat([x, neigh], dim=1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        neigh = self.aggregate(x, edge_index, num_nodes)\n",
    "        x = torch.cat([x, neigh], dim=1)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # L2 normalize\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleGraphSAGE(in_dim=7, hidden_dim=32, out_dim=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = SkipGramLoss(scale_factor=5.0)\n",
    "\n",
    "print(\"Mini Training Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "num_negatives = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Shuffle pairs\n",
    "    indices = np.random.permutation(len(positive_pairs))\n",
    "    \n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        if len(batch_indices) < batch_size // 2:\n",
    "            continue\n",
    "        \n",
    "        # Get batch pairs\n",
    "        batch_pairs = [positive_pairs[j] for j in batch_indices]\n",
    "        targets = torch.tensor([p[0] for p in batch_pairs])\n",
    "        contexts = torch.tensor([p[1] for p in batch_pairs])\n",
    "        \n",
    "        # Sample negatives\n",
    "        negatives = torch.tensor(\n",
    "            neg_sampler.sample_for_pairs(len(batch_pairs), num_negatives)\n",
    "        )\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(features, memory_view.edge_index)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(embeddings, targets, contexts, negatives)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1:2d} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the embeddings\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_embeddings = model(features, memory_view.edge_index)\n",
    "\n",
    "print(\"Embedding Quality Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check normalization\n",
    "norms = final_embeddings.norm(dim=1)\n",
    "print(f\"\\n1. Normalization check:\")\n",
    "print(f\"   All norms â‰ˆ 1.0: {torch.allclose(norms, torch.ones_like(norms), atol=1e-5)}\")\n",
    "\n",
    "# Check neighbor similarity vs random similarity\n",
    "edge_index = memory_view.edge_index\n",
    "\n",
    "# Neighbor similarities\n",
    "src_emb = final_embeddings[edge_index[0]]\n",
    "tgt_emb = final_embeddings[edge_index[1]]\n",
    "neighbor_sim = (src_emb * tgt_emb).sum(dim=1).mean().item()\n",
    "\n",
    "# Random similarities\n",
    "random_idx1 = torch.randint(0, final_embeddings.size(0), (1000,))\n",
    "random_idx2 = torch.randint(0, final_embeddings.size(0), (1000,))\n",
    "random_sim = (final_embeddings[random_idx1] * final_embeddings[random_idx2]).sum(dim=1).mean().item()\n",
    "\n",
    "print(f\"\\n2. Similarity check:\")\n",
    "print(f\"   Neighbor similarity: {neighbor_sim:.4f}\")\n",
    "print(f\"   Random similarity:   {random_sim:.4f}\")\n",
    "print(f\"   Gap:                 {neighbor_sim - random_sim:.4f}\")\n",
    "\n",
    "if neighbor_sim > random_sim + 0.05:\n",
    "    print(\"\\nâœ… Embeddings capture graph structure!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Gap is small - may need more training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: What You've Learned\n",
    "\n",
    "## The GDS-Style Training Pipeline\n",
    "\n",
    "### 1. Data Pipeline\n",
    "```\n",
    "Raw Graph â†’ Memory View â†’ Features â†’ PyG Dataset\n",
    "```\n",
    "\n",
    "### 2. Feature Extraction\n",
    "- 7-dimensional structural features\n",
    "- Continuous (log-normalized) + Binary features\n",
    "- Captures node roles in the graph\n",
    "\n",
    "### 3. Random Walk Generation\n",
    "- Start from each node\n",
    "- Randomly traverse neighbors\n",
    "- Captures structural proximity\n",
    "\n",
    "### 4. Pair Sampling\n",
    "- Context window extracts co-occurring pairs\n",
    "- Amplifies training signal (edges â†’ millions of pairs)\n",
    "\n",
    "### 5. Negative Sampling\n",
    "- Degree-biased: `P(node) âˆ degree^0.75`\n",
    "- Prevents embedding collapse\n",
    "\n",
    "### 6. Scale Factor (Critical!)\n",
    "- L2 normalized embeddings â†’ bounded dot products\n",
    "- Without scale: sigmoid stuck at [0.27, 0.73]\n",
    "- With scale=5.0: sigmoid can reach [0.007, 0.993]\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Random walks** convert graph structure into sequences\n",
    "2. **Co-occurrence** creates massive training signal\n",
    "3. **Skip-gram loss** learns from similarity/dissimilarity\n",
    "4. **Scale factor is essential** for normalized embeddings\n",
    "5. **Evaluation metrics > training loss** for quality assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercises\n",
    "\n",
    "## Exercise 1: Walk Parameters\n",
    "Experiment with different `walk_length` (40, 80, 160) and `walks_per_node` (5, 10, 20). How do they affect:\n",
    "- Number of training pairs?\n",
    "- Training time?\n",
    "- Final embedding quality?\n",
    "\n",
    "## Exercise 2: Context Window\n",
    "Try context windows of 2, 5, 10, and 20. What happens to the training signal and embedding quality?\n",
    "\n",
    "## Exercise 3: Scale Factor\n",
    "Train with scale factors 1.0, 3.0, 5.0, and 10.0. Plot the `pos_prob` and `neg_prob` curves.\n",
    "\n",
    "## Exercise 4: Negative Sampling Exponent\n",
    "Compare exponents 0.0, 0.5, 0.75, and 1.0. Which gives the best link prediction AUC?\n",
    "\n",
    "## Exercise 5: Feature Ablation\n",
    "Train with subsets of features (e.g., only degrees, only binary). Which features are most important?\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand how to build a GDS-style GraphSAGE training pipeline from scratch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
